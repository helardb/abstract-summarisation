{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"small\")\n",
    "result = model.transcribe(\"test.m4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\" So this is my quick, verbal summary of the paper sort of detailing the whisper. So the paper is called robust speech recognition via large scale weak supervision. So basically whisper is an automatic speech recognition system. It was developed by OpenAI and released in 2022. So it's highly versatile and very accurate speech to text model. So the paper goes through its architecture, how they pre-process the data, the training process and the detail of its performance across a range of speech recognition tasks. So a bit about the architecture, they call it an encode decode transformer. So it's similar to other large language models like OpenAI. It's very effective for sequential data. So it sort of splits up the input audio into 30 second chunks and first it converts it to log mail spectrogram and then they pass it to an encoder and then it's decoded to predict the corresponding text caption and they use all sorts of special tokens and it can perform a wide range of tasks. So it can identify what language is being spoken, it can translate that language into English or it can just transcribe that language as well. So it's based on 680,000 hours of data. This is mostly in English but they have about a third of that is in other languages as well. There's a wide range of other languages, roughly 100 or so. And the key part of this is that the training data is weakly supervised. So there are transcriptions available, so ideal outputs that are included with a lot of the audio samples but because of the huge scale of this there may be inconsistencies or errors in these that means that data is a bit noisy. So they use some other techniques to filter out a lot of this noise and make it as clean as possible. They claim that it is extremely robust across many different languages, different accents, different environments where things might be spoken like in a noisy background. It was designed to handle fast speech, multiple different speakers, people speaking on clearly as well. So the paper goes into a lot of detail on the performance of Whisper compared to other ASR programs. They claim it's particularly strong at zero shot transcription. So having a very first look at some audio or language that hasn't been trained or fine tuned on it performs very well. So the weak supervision means that there is comparatively little human interference in the training process. So it may be slightly misaligned or contain minor errors. So Whisper comes in a few different sizes ranging from Whisper Tiny to Whisper Large. And these vary in how computationally intensive they are because it's of a huge size. And the transformer based architecture also contributes to this. So it doesn't always outperform specialized ASR systems which have been optimized for a particular language or task. So in conclusion, the paper emphasizes that Whisper's strength comes from its scale and diversity of training data and its robust transformer architecture. Its ability to handle challenging, noisier accent speech along with its multilingual and multitask capabilities makes it a groundbreaking ASR system. The generalizability of Whisper is particularly emphasized as it can operate effectively across diverse audio inputs, accents and languages without requiring further fine tuning.\"}\n"
     ]
    }
   ],
   "source": [
    "print({result[\"text\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = result[\"text\"]\n",
    "prompt = f'''Summarise the following transcription of my voice notes on Whisper ASR: \"{transcript}\".'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████████████████████████████████████████| 4.66G/4.66G [07:09<00:00, 10.9MiB/s]\n",
      "Verifying: 100%|██████████████████████████████████████████████████████████████████| 4.66G/4.66G [00:06<00:00, 751MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Summary of transcript: \n",
      "  \n",
      "\n",
      "Here's the summary:\n",
      "\n",
      "The paper \"Robust Speech Recognition via Large Scale Weak Supervision\" discusses OpenAI's automatic speech recognition (ASR) model called Whisper, released in 2022. The system is highly versatile and accurate for transcribing spoken language into text. The architecture of Whisper involves an encode-decode transformer that splits input audio into 30-second chunks, converts it to a log mel spectrogram, and then predicts the corresponding text caption using special tokens.\n",
      "\n",
      "Whisper was trained on 680,000 hours of data, mostly in English but also including about one-third from other languages. The training process uses weak supervision, where transcriptions are available for some audio samples, but may contain inconsistencies or errors due to its large scale. To filter out noise, Whisper employs additional techniques.\n",
      "\n",
      "The paper highlights Whisper's robustness across various languages, accents, and environments, making it suitable for handling fast speech, multiple speakers, and noisy backgrounds. It also excels at zero-shot transcription\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "\n",
    "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\")\n",
    "\n",
    "output = model.generate(prompt)\n",
    "\n",
    "print(f' Summary of transcript: \\n {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
